{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11747-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1WLVkXVf7Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "61317489-a40e-4c5f-e0d8-5efb38df2025"
      },
      "source": [
        "import torch \n",
        "import nltk\n",
        "import pdb\n",
        "import numpy as np\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "\n",
        "from time import time\n",
        "from sklearn import metrics\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "nltk.download('wordnet')\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "%pdb off"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Automatic pdb calling has been turned OFF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6NMLOg6egpV",
        "colab_type": "text"
      },
      "source": [
        "## Global Argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOwyz_maej_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Argument():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 256\n",
        "        self.embedding_size = 300\n",
        "        self.hidden_size = 512\n",
        "        self.n_class = 16\n",
        "        self.dropout = 0.5\n",
        "        self.filter_num = 100\n",
        "        self.layer_num = 1\n",
        "        self.lr = 1e-3\n",
        "        self.wd = 1e-4\n",
        "        self.n_epochs = 10\n",
        "        self.iter_interval = 300\n",
        "        self.metric_list = ['accuracy']\n",
        "\n",
        "args = Argument()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNILZy8OPs4q",
        "colab_type": "text"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9zV9oZf-B4",
        "colab_type": "code",
        "outputId": "a15aafda-6bb2-4714-83fb-40e479a5bffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "%%sh\n",
        "wget http://phontron.com/data/topicclass-v1.tar.gz\n",
        "tar xvzf topicclass-v1.tar.gz topicclass\n",
        "wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "unzip glove.42B.300d.zip\n",
        "wc -l glove.42B.300d.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.42B.300d.txt\n",
            "glove.42B.300d.zip\n",
            "sample_data\n",
            "topicclass\n",
            "topicclass-v1.tar.gz\n",
            "topicclass-v1.tar.gz.1\n",
            "topicclass-v1.tar.gz.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7jsJS_yMHXR",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQqbF_CFT7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedEmbedding():\n",
        "    def __init__(self, embedding_size):\n",
        "        self.embedding_dict = {}\n",
        "        self.embedding_size = embedding_size\n",
        "    \n",
        "    def load(self, embedding_file, existing_tokens):\n",
        "        start_time = time()\n",
        "        existing_tokens = set(existing_tokens)\n",
        "        print(\"Loading embedding from file...\")\n",
        "\n",
        "        with open(embedding_file, 'r') as f:\n",
        "            for line in f:\n",
        "                line_split = line.split()\n",
        "                token, vec = line_split[0], line_split[1:]\n",
        "\n",
        "                if token not in existing_tokens:\n",
        "                    continue\n",
        "\n",
        "                vec = list(map(lambda x: float(x), vec))\n",
        "                assert token not in self.embedding_dict\n",
        "                self.embedding_dict[token] = vec\n",
        "\n",
        "        print(\"Loading finished in {:.1f}s\".format(time() - start_time))\n",
        "\n",
        "    def get_embedding_words(self):\n",
        "        return list(self.embedding_dict.keys())\n",
        "    \n",
        "    def get_embedding_size(self):\n",
        "        return self.embedding_size\n",
        "\n",
        "    def get_embedding_weight_matrix(self, tokens_ls):\n",
        "        weight_matrix = np.zeros((len(tokens_ls), self.embedding_size))\n",
        "        for i, token in enumerate(tokens_ls):\n",
        "            try:\n",
        "                weight_matrix[i] = self.embedding_dict[token]\n",
        "            except KeyError:\n",
        "                weight_matrix[i] = np.random.normal(scale=0.6, \n",
        "                                                     size=(self.embedding_size, ))\n",
        "        return weight_matrix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embedding_dict)\n",
        "\n",
        "\n",
        "class Vocabulary():\n",
        "    def __init__(self):\n",
        "        self.token2index = {'<UNK>': 0, '<PAD>': 1}\n",
        "        self.token_ls = ['<UNK>', '<PAD>']\n",
        "        self.token_cnt = 2\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token not in self.token2index:\n",
        "            self.token2index[token] = self.token_cnt\n",
        "            self.token_ls.append(token)\n",
        "            self.token_cnt += 1\n",
        "        assert self.token_cnt == len(self.token_ls) == len(self.token2index)\n",
        "\n",
        "    def add_token_ls(self, token_ls):\n",
        "        for token in token_ls:\n",
        "            self.add_token(token)\n",
        "\n",
        "    def get_token_ls(self):\n",
        "        return self.token_ls\n",
        "\n",
        "    def get_token2index(self):\n",
        "        return self.token2index\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.token_cnt\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"Vocabulary of {} tokens\".format(self.token_cnt)\n",
        "\n",
        "\"\"\"\n",
        "The main class that will be used to process the data for topic classification.\n",
        "\"\"\"\n",
        "class DataProcessor():\n",
        "    def __init__(self, lemmatizer):\n",
        "        self.vocab = Vocabulary()\n",
        "        self.lemmatizer = lemmatizer\n",
        "        self.label2index = {}\n",
        "\n",
        "    \"\"\" \n",
        "    Parse the input txt file and return a processed format\n",
        "\n",
        "    Args:\n",
        "        file_name: the name of the input file that need to be parsed\n",
        "        mode: \"train\" / \"validation\" / \"test\"\n",
        "              \"train\": parsing the training data, build vocabulary and \n",
        "                       label mapping on the fly.\n",
        "              \"validation\": parsing the validation data.\n",
        "              \"test\": parsing the test data.\n",
        "    Return:\n",
        "            (label_ls, tokens_ls)\n",
        "            label_ls: A list of topic labels in index format. \n",
        "                      [topic_idx_1, topic_idx_2, ...]\n",
        "            tokens_id_ls: A list of tokens that has been converted to index\n",
        "                          based on the vocabulary build on training set.\n",
        "                          [[t11, t12, t13, ...], [t21, t22, ...], ...]\n",
        "    \"\"\"\n",
        "    def process(self, file_name, mode):\n",
        "        start_time = time()\n",
        "        f_read = open(file_name, 'r')\n",
        "        success_line_cnt = 0\n",
        "        fail_line_cnt = 0\n",
        "        topic_cnt = 0\n",
        "        label_ls = []\n",
        "        tokens_id_ls = []\n",
        "\n",
        "        for line in f_read:\n",
        "            try:\n",
        "                assert len(line.split('|||')) == 2\n",
        "                success_line_cnt += 1\n",
        "            except:\n",
        "                fail_line_cnt += 1\n",
        "                continue\n",
        "\n",
        "            splited_line = line.split('|||')\n",
        "            topic= splited_line[0].lower().strip()\n",
        "            text = splited_line[1].lower().strip()\n",
        "\n",
        "            token_ls = self.tokenize(text)\n",
        "\n",
        "            if mode == \"train\":\n",
        "                # update label-to-index mapping\n",
        "                if topic not in self.label2index:\n",
        "                    self.label2index[topic] = topic_cnt\n",
        "                    topic_cnt += 1\n",
        "\n",
        "                # update vocabulary\n",
        "                self.vocab.add_token_ls(token_ls)\n",
        "\n",
        "            # append tokens and label in a index version\n",
        "            tokens_id_ls.append(self.get_tokens_id(token_ls))\n",
        "            if mode != \"test\":\n",
        "                label_ls.append(self.get_label_idx(topic))\n",
        "\n",
        "        print(\"Data processing on {} data finished in {:.1f}s. \"\n",
        "              \"[{} line sucessful, {} line failed]\".format(\n",
        "                mode, time() - start_time, success_line_cnt, fail_line_cnt))\n",
        "\n",
        "        return label_ls, tokens_id_ls\n",
        "\n",
        "\n",
        "    def get_label_idx(self, topic):\n",
        "        try:\n",
        "            label_idx = self.label2index[topic]\n",
        "        except:\n",
        "            if topic == \"media and darama\":\n",
        "                label_idx = self.label2index[\"media and drama\"]\n",
        "            else:\n",
        "                raise ValueError(\"the topic {} is not defined \"\n",
        "                                 \"in the training set\".format(topic))\n",
        "\n",
        "        return label_idx\n",
        "\n",
        "    def get_tokens_id(self, token_ls):\n",
        "        tokens_id = []\n",
        "        for token in token_ls:\n",
        "            try:\n",
        "                tokens_id.append(self.vocab.get_token2index()[token])\n",
        "            except:\n",
        "                tokens_id.append(self.vocab.get_token2index()['<UNK>'])\n",
        "\n",
        "        assert len(tokens_id) == len(token_ls)\n",
        "        return tokens_id\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        token_ls = text.split()\n",
        "        token_ls = list(map(self.tokenize_function, token_ls))\n",
        "        return token_ls\n",
        "\n",
        "    def tokenize_function(self, token):\n",
        "        token = token.strip()\n",
        "        if self.lemmatizer is not None:\n",
        "            token = self.lemmatizer.lemmatize(token)\n",
        "            token = self.lemmatizer.lemmatize(token, 'v')\n",
        "            token = token.strip()\n",
        "        return token\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.vocab\n",
        "\n",
        "    def get_label2index(self):\n",
        "        return self.label2index\n",
        "    \n",
        "    def get_label_ls(self):\n",
        "        label_ls = [_ for i in range(len(self.label2index))]\n",
        "        for label, index in self.label2index.items():\n",
        "            label_ls[index] = label \n",
        "        return label_ls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHK_BO_9L64O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "b7538b11-0b0b-413c-d296-d77a35795b49"
      },
      "source": [
        "data_processor = DataProcessor(WordNetLemmatizer())\n",
        "train_label_ls, train_tokens_id_ls = data_processor.process(\n",
        "    'topicclass/topicclass_train.txt', 'train')\n",
        "valid_label_ls, valid_tokens_id_ls = data_processor.process(\n",
        "    'topicclass/topicclass_valid.txt', 'validation')\n",
        "test_label_ls, test_tokens_id_ls = data_processor.process(\n",
        "    'topicclass/topicclass_test.txt', 'test')\n",
        "vocabulary = data_processor.get_vocab()\n",
        "\n",
        "word_embedding = PretrainedEmbedding(args.embedding_size)\n",
        "word_embedding.load('glove.42B.300d.txt', vocabulary.get_token_ls())\n",
        "weight_matrix = word_embedding.get_embedding_weight_matrix(vocabulary.get_token_ls())\n",
        "\n",
        "print(\"Overall vocabulary size is {}\".format(vocabulary.get_vocab_size()))\n",
        "print(\"Overall topic number is {}\".format(len(data_processor.get_label2index())))\n",
        "print(\"{} of {} ({:.0f}%) tokens has pretrained embeddings\".format(\n",
        "    len(word_embedding.get_embedding_words()), vocabulary.get_vocab_size(),\n",
        "    len(word_embedding.get_embedding_words()) / vocabulary.get_vocab_size() * 100\n",
        "))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data processing on train data finished in 49.7s. [253909 line sucessful, 0 line failed]\n",
            "Data processing on validation data finished in 0.1s. [643 line sucessful, 0 line failed]\n",
            "Data processing on test data finished in 0.1s. [697 line sucessful, 0 line failed]\n",
            "Overall vocabulary size is 104817\n",
            "Overall topic number is 16\n",
            "85031 of 104817 (81%) tokens has pretrained embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A95hpAjTzxj",
        "colab_type": "text"
      },
      "source": [
        "## Initialize PyTorch Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWgDnnlqNsEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopicTextData(Dataset):\n",
        "    \"\"\"\n",
        "    The dataset for the topic classification.\n",
        "\n",
        "    Args:\n",
        "        tokens_id_ls: A list of tokens that has been converted to index\n",
        "                        based on the vocabulary build on training set.\n",
        "                        [[t11, t12, t13, ...], [t21, t22, ...], ...]\n",
        "        label_ls: A list of topic labels in index format. \n",
        "                    [topic_idx_1, topic_idx_2, ...]\n",
        "    \"\"\"\n",
        "    def __init__(self, tokens_id_ls, label_ls):\n",
        "        self.tokens_id_ls = tokens_id_ls\n",
        "        self.label_ls = label_ls\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens_id_ls)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tokens_id_tensor = torch.LongTensor(self.tokens_id_ls[index])\n",
        "\n",
        "        if self.label_ls is None or len(self.label_ls) == 0:\n",
        "            return tokens_id_tensor, 0\n",
        "\n",
        "        label = self.label_ls[index]\n",
        "        return tokens_id_tensor, label\n",
        "\n",
        "def collate_with_padding(batch):\n",
        "    tokens_id_tensor_ls = [item[0] for item in batch]\n",
        "    label_ls = [item[1] for item in batch]\n",
        "\n",
        "    tokens_id_tensor_padded = (pad_sequence(tokens_id_tensor_ls, \n",
        "                                            padding_value=1)\n",
        "                              .transpose(1, 0)) # (B, max_sent_len)\n",
        "    label_tensor = torch.LongTensor(label_ls)\n",
        "\n",
        "    return tokens_id_tensor_padded, label_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY9c4ubqRWOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader_params = dict(shuffle=True, \n",
        "                           batch_size=args.batch_size,\n",
        "                           num_workers=4,\n",
        "                           pin_memory=True,\n",
        "                           drop_last=False,\n",
        "                           collate_fn=collate_with_padding)\n",
        "\n",
        "valid_loader_params = dict(shuffle=False, \n",
        "                           batch_size=args.batch_size,\n",
        "                           num_workers=4,\n",
        "                           pin_memory=True,\n",
        "                           drop_last=False,\n",
        "                           collate_fn=collate_with_padding)\n",
        "\n",
        "train_data = TopicTextData(train_tokens_id_ls, train_label_ls)\n",
        "valid_data = TopicTextData(valid_tokens_id_ls, valid_label_ls)\n",
        "test_data = TopicTextData(test_tokens_id_ls, test_label_ls)\n",
        "train_dataloader = DataLoader(train_data, **train_loader_params)\n",
        "valid_dataloader = DataLoader(valid_data, **valid_loader_params)\n",
        "test_dataloader = DataLoader(test_data, **valid_loader_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZrvvi6Wo0r0",
        "colab_type": "text"
      },
      "source": [
        "## Define Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l5Bxr0JlbtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    \"\"\"The base CNN block for text classificaton.\n",
        "\n",
        "    It contains a 1-D convolution over time, as well as \n",
        "    max-pooling over time\n",
        "\n",
        "    Input:\n",
        "        token_embeddings (Tensor): B * E * T embeddings of input sentence.\n",
        "    Output:\n",
        "        conv_features (Tensor): B * FILTER_NUM convoluted features.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, filter_size, filter_num, layer_num):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv_layer = self.construct_conv_layer(embedding_size, \n",
        "                                                    filter_size, \n",
        "                                                    filter_num, \n",
        "                                                    layer_num)\n",
        "\n",
        "    def forward(self, token_embeddings):\n",
        "        conv_feats = self.conv_layer(token_embeddings)  # (B, E, T) -> (B, K, T')\n",
        "        maxpool_feats, _ = torch.max(conv_feats, dim=2)  # (B, K, T') -> (B, K)\n",
        "        return maxpool_feats\n",
        "\n",
        "    def construct_conv_layer(self, embedding_size, filter_size, \n",
        "                             filter_num, layer_num):\n",
        "        if layer_num < 2:\n",
        "            return  nn.Conv1d(in_channels=embedding_size,\n",
        "                              out_channels=filter_num,\n",
        "                              kernel_size=filter_size)\n",
        "        else:\n",
        "            module_ls = []\n",
        "            module_ls.append(\n",
        "                nn.Conv1d(in_channels=embedding_size,\n",
        "                          out_channels=filter_num,\n",
        "                          kernel_size=filter_size)\n",
        "            )\n",
        "            for i in range(layer_num - 1):\n",
        "                module_ls.append(\n",
        "                    nn.Conv1d(in_channels=filter_num,\n",
        "                              out_channels=filter_num,\n",
        "                              kernel_size=filter_size)\n",
        "                )\n",
        "\n",
        "            return nn.Sequential(*module_ls)\n",
        "\n",
        "\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self, args, voc, embedding_weight_matrix=None):\n",
        "        super(DeepCNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=voc.get_vocab_size(),\n",
        "                                      embedding_dim=args.embedding_size)\n",
        "        if embedding_weight_matrix is not None:\n",
        "            embedding_weight_matrix = torch.Tensor(embedding_weight_matrix)\n",
        "            self.embedding.load_state_dict({'weight': embedding_weight_matrix})\n",
        "            self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.CNN_filter_3 = CNNBlock(args.embedding_size, 3, args.filter_num, \n",
        "                                     args.layer_num)\n",
        "        self.CNN_filter_4 = CNNBlock(args.embedding_size, 4, args.filter_num, \n",
        "                                     args.layer_num)\n",
        "        self.CNN_filter_5 = CNNBlock(args.embedding_size, 5, args.filter_num,\n",
        "                                     args.layer_num)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.BatchNorm1d(args.filter_num * 3),\n",
        "            nn.Dropout(args.dropout),\n",
        "            nn.Linear(args.filter_num * 3, args.hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(args.hidden_size),\n",
        "            nn.Dropout(args.dropout),\n",
        "            nn.Linear(args.hidden_size, args.n_class),\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens_id):\n",
        "        tokens_embedding = self.embedding(tokens_id).permute(0, 2, 1) # (B, E, T)\n",
        "        conv_feats_filter_size_3 = self.CNN_filter_3(tokens_embedding) # (B, K)\n",
        "        conv_feats_filter_size_4 = self.CNN_filter_4(tokens_embedding) # (B, K)\n",
        "        conv_feats_filter_size_5 = self.CNN_filter_5(tokens_embedding) # (B, K)\n",
        "        conv_feats_concat = torch.cat([conv_feats_filter_size_3,\n",
        "                                       conv_feats_filter_size_4,\n",
        "                                       conv_feats_filter_size_5], dim=1)        \n",
        "        pred_result = self.linear(conv_feats_concat)\n",
        "\n",
        "        return pred_result\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQo8w1ok8dH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "a1396a97-0487-4f51-cf0c-d75193e357f4"
      },
      "source": [
        "model = DeepCNN(args, vocabulary, weight_matrix).to(device)\n",
        "model"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepCNN(\n",
              "  (embedding): Embedding(104817, 300)\n",
              "  (CNN_filter_3): CNNBlock(\n",
              "    (conv_layer): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
              "  )\n",
              "  (CNN_filter_4): CNNBlock(\n",
              "    (conv_layer): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
              "  )\n",
              "  (CNN_filter_5): CNNBlock(\n",
              "    (conv_layer): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
              "  )\n",
              "  (linear): Sequential(\n",
              "    (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Linear(in_features=300, out_features=512, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=16, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8YTV1aCNbEo",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUMl9Ei9LnTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKm5wnjkOCVq",
        "colab_type": "text"
      },
      "source": [
        "## Model Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgvvfmjKL4oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Evaluator():\n",
        "    def evaluate(self, y_true, y_pred_proba, metric_list):\n",
        "        y_pred = np.argmax(y_pred_proba, -1)\n",
        "        output = {}\n",
        "        \n",
        "        if 'accuracy' in metric_list:\n",
        "            output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
        "        \n",
        "        if 'micro_f1' in metric_list:\n",
        "            output['micro-f1'] = metrics.f1_score(y_true, y_pred, \n",
        "                                                  average='micro')\n",
        "\n",
        "        if 'macro_f1' in metric_list:\n",
        "            output['macro-f1'] = metrics.f1_score(y_true, y_pred, \n",
        "                                                  average='macro')\n",
        "\n",
        "        if 'confusion_matrix' in metric_list:\n",
        "            output['confusion_matrix'] = str(metrics.confusion_matrix(\n",
        "                    y_true, y_pred))\n",
        "        return output\n",
        "\n",
        "    def get_metric_str(self, metric_dict):\n",
        "        info_str = \"\"\n",
        "        for metric, value in metric_dict.items():\n",
        "            info_str += \"{}: {:.3f}, \".format(metric, value)\n",
        "        return info_str[:-2]\n",
        "\n",
        "class ModelRunner():\n",
        "    def __init__(self, args, model, optim, criterion, device, lr_scheduler):\n",
        "        self.n_epochs = args.n_epochs\n",
        "        self.metric_list = args.metric_list\n",
        "        self.iter_interval = args.iter_interval\n",
        "        self.device = device\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optim\n",
        "        self.criterion = criterion\n",
        "        self.evaluator = Evaluator()\n",
        "        self.time = time()\n",
        "        self.running_loss = 0\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "\n",
        "    def train(self, train_dataloader, valid_dataloader=None):\n",
        "        self.time = time()\n",
        "        for epoch in range(self.n_epochs):\n",
        "            self.train_epoch(epoch, train_dataloader)\n",
        "            val_loss = self.print_valid_process(valid_dataloader)\n",
        "            self.lr_scheduler.step(val_loss)\n",
        "    \n",
        "    def print_valid_process(self, valid_dataloader):\n",
        "        if valid_dataloader is None:\n",
        "            return \n",
        "\n",
        "        self.model.eval()\n",
        "        all_true_labels = []\n",
        "        all_pred_labels = []\n",
        "        val_loss, n_samples = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for feat, label in valid_dataloader:\n",
        "                batch_size = len(label)\n",
        "                feat, label = feat.to(self.device), label.to(self.device)\n",
        "                pred_topic_proba = self.model(feat)  # (B, n_class)\n",
        "                loss = self.criterion(pred_topic_proba, label)\n",
        "                \n",
        "                val_loss += loss.item() * batch_size\n",
        "                n_samples += batch_size\n",
        "                all_true_labels.extend(label.cpu().tolist())\n",
        "                all_pred_labels.extend(pred_topic_proba.cpu().tolist())\n",
        "        \n",
        "        valid_metrics = self.evaluator.evaluate(np.array(all_true_labels),\n",
        "                                                np.array(all_pred_labels),\n",
        "                                                self.metric_list)\n",
        "        metric_str = self.evaluator.get_metric_str(valid_metrics)\n",
        "        print(\"[Evaluation] loss: {:.3f} {}\".format(\n",
        "            val_loss / n_samples, metric_str\n",
        "        ))\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        self.model.eval() \n",
        "        input_data = input_data.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_topic_proba = self.model(input_data)\n",
        "        return np.argmax(pred_topic_proba.cpu().numpy(), axis=1)\n",
        "\n",
        "    def train_epoch(self, epoch, train_dataloader):\n",
        "        self.model.train()\n",
        "        num_iter_per_epoch = len(train_dataloader)\n",
        "        all_labels, all_predictions = [], []\n",
        "        self.running_loss = 0\n",
        "        n_samples = 0\n",
        "\n",
        "        for idx, (feat, label) in enumerate(train_dataloader):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            batch_size = len(label)\n",
        "            feat, label = feat.to(self.device), label.to(self.device)\n",
        "            pred_topic_proba = self.model(feat)  # (B, n_class)\n",
        "            pred_topics = torch.argmax(pred_topic_proba, dim=1)\n",
        "\n",
        "            loss = self.criterion(pred_topic_proba, label)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.running_loss += loss.item() * batch_size\n",
        "            n_samples += batch_size\n",
        "\n",
        "            all_labels.extend(label.cpu().tolist())\n",
        "            all_predictions.extend(pred_topic_proba.cpu().tolist())\n",
        "\n",
        "            self.print_train_process(epoch, idx, num_iter_per_epoch, \n",
        "                                     n_samples, all_labels, all_predictions)\n",
        "\n",
        "    def print_train_process(self, epoch, idx, num_iter_per_epoch, n_samples,\n",
        "                            all_labels, all_predictions):\n",
        "        if (idx + 1) % self.iter_interval == 0:\n",
        "            train_metrics = self.evaluator.evaluate(np.array(all_labels),\n",
        "                                                    np.array(all_predictions),\n",
        "                                                    self.metric_list)\n",
        "            metric_str = self.evaluator.get_metric_str(train_metrics)\n",
        "            train_info = \"Epoch: {}/{}, iter: {}/{}, loss: {:.3f}, \"\\\n",
        "                         \"{} [{:.1f}s]\".format(epoch + 1, self.n_epochs, \n",
        "                                              idx + 1, num_iter_per_epoch, \n",
        "                                              self.running_loss / n_samples, \n",
        "                                              metric_str, time() - self.time)\n",
        "            print(train_info)\n",
        "            self.time = time()\n",
        "\n",
        "    def get_prediction(self, dataloader, label_ls):\n",
        "        self.model.eval() \n",
        "        pred_id_ls = []\n",
        "        pred_topic_ls = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for feat, _ in dataloader:\n",
        "                feat = feat.to(device)\n",
        "                pred_topic_proba = self.model(feat)\n",
        "                pred_id_ls.extend(\n",
        "                    np.argmax(pred_topic_proba.cpu().numpy(), axis=1).tolist())\n",
        "        \n",
        "        for topic_id in pred_id_ls:\n",
        "            pred_topic_ls.append(label_ls[topic_id])\n",
        "        \n",
        "        return pred_topic_ls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQjda5KKL79O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "d1781bd3-50c7-4316-8e7a-d8e58f936b89"
      },
      "source": [
        "model_runner = ModelRunner(args, model, optimizer, criterion, device,\n",
        "                           ReduceLROnPlateau(optimizer, \n",
        "                                             mode='min', \n",
        "                                             patience=1,\n",
        "                                             factor=0.2))\n",
        "model_runner.train(train_dataloader, valid_dataloader)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10, iter: 300/992, loss: 0.613, accuracy: 0.799 [4.0s]\n",
            "Epoch: 1/10, iter: 600/992, loss: 0.614, accuracy: 0.799 [3.6s]\n",
            "Epoch: 1/10, iter: 900/992, loss: 0.617, accuracy: 0.798 [4.4s]\n",
            "[Evaluation] loss: 0.563 accuracy: 0.829\n",
            "Epoch: 2/10, iter: 300/992, loss: 0.600, accuracy: 0.803 [5.8s]\n",
            "Epoch: 2/10, iter: 600/992, loss: 0.606, accuracy: 0.802 [3.6s]\n",
            "Epoch: 2/10, iter: 900/992, loss: 0.605, accuracy: 0.802 [3.7s]\n",
            "[Evaluation] loss: 0.561 accuracy: 0.827\n",
            "Epoch: 3/10, iter: 300/992, loss: 0.585, accuracy: 0.806 [6.8s]\n",
            "Epoch: 3/10, iter: 600/992, loss: 0.590, accuracy: 0.806 [3.6s]\n",
            "Epoch: 3/10, iter: 900/992, loss: 0.594, accuracy: 0.804 [3.6s]\n",
            "[Evaluation] loss: 0.569 accuracy: 0.827\n",
            "Epoch: 4/10, iter: 300/992, loss: 0.573, accuracy: 0.811 [6.5s]\n",
            "Epoch: 4/10, iter: 600/992, loss: 0.576, accuracy: 0.811 [3.5s]\n",
            "Epoch: 4/10, iter: 900/992, loss: 0.583, accuracy: 0.809 [3.6s]\n",
            "[Evaluation] loss: 0.568 accuracy: 0.824\n",
            "Epoch: 5/10, iter: 300/992, loss: 0.557, accuracy: 0.816 [6.6s]\n",
            "Epoch: 5/10, iter: 600/992, loss: 0.565, accuracy: 0.813 [3.6s]\n",
            "Epoch: 5/10, iter: 900/992, loss: 0.564, accuracy: 0.813 [3.7s]\n",
            "[Evaluation] loss: 0.562 accuracy: 0.834\n",
            "Epoch: 6/10, iter: 300/992, loss: 0.555, accuracy: 0.818 [6.0s]\n",
            "Epoch: 6/10, iter: 600/992, loss: 0.555, accuracy: 0.817 [4.2s]\n",
            "Epoch: 6/10, iter: 900/992, loss: 0.558, accuracy: 0.815 [3.6s]\n",
            "[Evaluation] loss: 0.564 accuracy: 0.832\n",
            "Epoch: 7/10, iter: 300/992, loss: 0.550, accuracy: 0.818 [5.9s]\n",
            "Epoch: 7/10, iter: 600/992, loss: 0.550, accuracy: 0.818 [4.3s]\n",
            "Epoch: 7/10, iter: 900/992, loss: 0.551, accuracy: 0.818 [3.5s]\n",
            "[Evaluation] loss: 0.562 accuracy: 0.832\n",
            "Epoch: 8/10, iter: 300/992, loss: 0.547, accuracy: 0.819 [5.8s]\n",
            "Epoch: 8/10, iter: 600/992, loss: 0.548, accuracy: 0.818 [3.5s]\n",
            "Epoch: 8/10, iter: 900/992, loss: 0.549, accuracy: 0.818 [4.4s]\n",
            "[Evaluation] loss: 0.563 accuracy: 0.834\n",
            "Epoch: 9/10, iter: 300/992, loss: 0.547, accuracy: 0.818 [5.8s]\n",
            "Epoch: 9/10, iter: 600/992, loss: 0.550, accuracy: 0.818 [3.5s]\n",
            "Epoch: 9/10, iter: 900/992, loss: 0.549, accuracy: 0.818 [3.6s]\n",
            "[Evaluation] loss: 0.564 accuracy: 0.830\n",
            "Epoch: 10/10, iter: 300/992, loss: 0.547, accuracy: 0.818 [6.5s]\n",
            "Epoch: 10/10, iter: 600/992, loss: 0.549, accuracy: 0.818 [3.5s]\n",
            "Epoch: 10/10, iter: 900/992, loss: 0.549, accuracy: 0.818 [3.5s]\n",
            "[Evaluation] loss: 0.563 accuracy: 0.832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJJoclBSH094",
        "colab_type": "text"
      },
      "source": [
        "## Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H4fDvDRlw-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_pred_topic_ls = model_runner.get_prediction(valid_dataloader, data_processor.get_label_ls())\n",
        "test_pred_topic_ls = model_runner.get_prediction(test_dataloader, data_processor.get_label_ls())\n",
        "\n",
        "with open('dev_results.txt', 'w') as f:\n",
        "    for topic in valid_pred_topic_ls:\n",
        "        f.write(topic + '\\n')\n",
        "\n",
        "with open('test_results.txt', 'w') as f:\n",
        "    for topic in test_pred_topic_ls:\n",
        "        f.write(topic + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}